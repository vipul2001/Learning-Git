{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BicycleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38YNt0i_jtmC",
        "colab_type": "text"
      },
      "source": [
        "<h1><b>BicycleGAN</b><i> (Implementation in pytorch)</i></h1>\n",
        "\n",
        "> <h2><b>Multimodal Image-to-Image Translation</b><br>\n",
        "\n",
        "![BicycleGAN](img/bicyclegan.png)\n",
        "<h2>Introduction</h2>\n",
        "<h6>\n",
        "Deep learning techniques have made rapid progress in conditional image generation. However, most techniques in this space have focused on generating a single result. Our aim is to generate a distribution of output images given an input image.<br><br>\n",
        "Mapping from a high-dimensional input to a high-dimensional output distribution is challenging. A common approach to representing multimodality is learning a low-dimensional latent code, which should represent aspects of the possible outputs not contained in the input image.  At inference time,\n",
        "a deterministic generator uses the input image, along with stochastically sampled latent codes, to produce randomly sampled outputs.\n",
        "</h6>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "<h2>Why BicycleGAN?</h2>\n",
        "A common problem in existing methods is mode collapse, where only a small number of real samples get represented in the output.\n",
        "\n",
        ">  **Mode Collapse** <br>\n",
        "Real life data distribution are multimodal. For example, **MNIST** dataset has 10 major modes from 0 to 9. When mode collapses, very few modes are generated.\n",
        "You can simply understand it as lack of variety. However complete collapse doesn't occur often whereas partial collapse is common. Given figure explains it all. \n",
        "\n",
        "![Mode Collapse](img/mode_collapse.png)<br>\n",
        "Top row produces all the 10 modes of Mnist whereas bottom row produces only single mode (digit '6' ).<br><br>\n",
        "BicycleGan proposes a bijection between the output and latent space.<br>\n",
        "Not only the direct task of mapping the latent code (along with the input) to the output is performed but also jointly we learn an encoder from the output back to the latent space. This discourages two different latent codes from generating the same output (non-injective mapping) i.e. preventing **mode collapse**\n",
        "\n",
        "---\n",
        "<br>\n",
        "<h2>What BicycleGAN does?</h2>\n",
        "<h6>\n",
        "Goal is to learn a multi-modal mapping between two image domains, for example, edges and photographs, or night and day images, etc.\n",
        "Consider the input domain  <b>A ⊂ R$^{H×W×3}$</b> , which is to be mapped to an output domain <b>B ⊂ R$^{H×W×3}$</b>.( For example, consider A as edges and consider B as photographs made using those edges )<br><br>\n",
        "We are given a dataset of paired instances from these domains, (A∈A, B∈B) which is representative of a joint distribution p(A, B). It is important to note that there could be multiple plausible paired instances B that would correspond to\n",
        "an input instance A, but the training dataset usually contains only one such pair. However, given a new instance A during test time, our model should be able to generate a diverse set of output $\\hat{B}$ 's, corresponding to different modes in the distribution p(B|A).<br><br>\n",
        "We would like to learn the mapping that could sample the output $\\hat{B}$  \n",
        "from true conditional distribution given A, and produce results which are both diverse and realistic.<br><br>\n",
        "To do so, we learn a low-dimensional latent space z ∈ R$^{z}$, which encapsulates the ambiguous aspects of the\n",
        "output mode which are not present in the input image. For example, a sketch of a shoe could map to a variety of colors and textures, which could get compressed in this latent code. We then learn a deterministic mapping G : (A, z) → B to the output. To enable stochastic sampling, we desire the latent code vector z to be drawn from some prior distribution p(z); we use a standard Gaussian distribution N (0, I) in this work.\n",
        "</h6>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "<h3>Our model consists of 2 parts</h3>\n",
        "\n",
        ">  Conditional Variational Autoencoder GAN: cVAE-GAN (1st part of model)\n",
        " (**B** → **z** → $\\hat{B}$)\n",
        "\n",
        "<br>\n",
        "\n",
        "![cVAE-GAN](img/cvae.png)\n",
        "<br>\n",
        "\n",
        "\n",
        "*  The ground truth B is directly mapped with latent code(z) using an encoder E.\n",
        "*  The generator G then uses both the latent code and the input image A to synthesize the desired output $\\hat{B}$.\n",
        "*  The overall model can be easily understood as the reconstruction of B, with latent encoding z concatenated with the paired A in the middle, similar to an autoencoder.\n",
        "* The distribution Q(z|B) of latent code z (output of the encoder E) is dealt with a Gaussian assumption, $Q(\\mathrm{z}|\\mathrm{B})=E(\\mathrm{B})$.<br><br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> <h4><b>cVAE-GAN objective, a conditional version of the VAE-GAN</b></h4><br>\n",
        "$$G^{*},\\displaystyle \\ E^{*}=\\arg\\min_{G,E}\\max_{D}\\ \\mathcal{L}_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}^{\\mathrm{V}\\mathrm{A}\\mathrm{E}}(G,\\ D,\\ E)+\\lambda \\mathcal{L}{1^{\\mathrm{V}\\mathrm{A}\\mathrm{E}}}(G,\\ E)+\\lambda_{\\mathrm{K}\\mathrm{L}}\\mathcal{L}_{\\mathrm{K}\\mathrm{L}}(E)$$\n",
        "\n",
        "<h3>where</h3> $$\\mathcal{L}_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}^{\\mathrm{V}\\mathrm{A}\\mathrm{E}}=\\mathrm{E}_{\\mathrm{A},\\mathrm{B}\\sim p(\\mathrm{A},\\mathrm{B})}[\\log(D(\\mathrm{A},\\ \\mathrm{B}))]+\\mathrm{E}_{\\mathrm{A},\\mathrm{B}\\sim p(\\mathrm{A},\\mathrm{B}),\\mathrm{z}\\sim E(\\mathrm{B})}[\\log(1-D(\\mathrm{A},\\ G(\\mathrm{A},\\ \\mathrm{z})))]$$\n",
        "\n",
        "* <h5>This is the typical loss function of GAN where Generator and Discriminator play a min-max game. Here Generator tries to fool the Discriminator whereas Discriminator tries to distinct the images generated by the Generator from the original ones.</h5>\n",
        "<br><br>\n",
        "\n",
        "$$\\mathcal{L}_{1}^{\\mathrm{V}\\mathrm{A}\\mathrm{E}}(G)= \\mathrm{E}_{\\mathrm{A},\\mathrm{B}\\sim p(\\mathrm{A},\\mathrm{B}),\\mathrm{z}\\sim E(\\mathrm{B})}||\\mathrm{B}-G(\\mathrm{A},\\ \\mathrm{z})||_{1}$$\n",
        "* <h5>To encourage the output of the generator to match the input as well as stabilize the training, we use an  $\\ell_{1}$ loss between the output and the ground truth image.\n",
        "<br><br><br>\n",
        "\n",
        "$$\\mathcal{L}_{\\mathrm{K}\\mathrm{L}}(E)=\\mathrm{E}_{\\mathrm{B}\\sim p(\\mathrm{B})}[\\mathcal{D}_{\\mathrm{K}\\mathrm{L}}(E(\\mathrm{B})||\\mathcal{N}(0,\\ I))]$$\n",
        "* <h6>The latent distribution encoded by $E(B)$ is encouraged to be close to a random Gaussian to enable sampling at inference time, when $\\mathrm{B}$ is not known. </h6><b>Here  $$\\displaystyle \\mathcal{D}_{\\mathrm{K}\\mathrm{L}}(p||q)=-\\int p(z)\\log\\frac{p(z)}{q(z)}dz$$\n",
        " \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<br><br>\n",
        ">Consider the deterministic version of this approach, i.e., dropping KLdivergence and encoding z = E(B). It is called cAE-GAN .\n",
        "There is no guarantee in cAE-GAN on the distribution of the latent space z, which makes the test-time\n",
        "sampling of z difficult.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "---\n",
        "\n",
        ">  Conditional Latent Regressor GAN: cLR-GAN (${z}$ → $\\hat{B}$ → $\\hat{z}$)\n",
        "<br>(2nd part of the model)\n",
        "\n",
        "<br>\n",
        "\n",
        "![cLR-GAN](img/clr.jpg)\n",
        "<br>\n",
        "\n",
        "* A randomly drawn latent code z is recovered with $\\hat{\\mathrm{z}}=E(G(\\mathrm{A},\\ \\mathrm{z}))$\n",
        "*  Encoder E here is producing a point estimate for $\\hat{\\mathrm{z}}$, whereas the encoder in the previous section was predicting a Gaussian distribution.\n",
        "<br><br>\n",
        "> <h4><b>cLR-GAN objective function</b></h4>\n",
        "<br>\n",
        "$G^{*},\\displaystyle \\ E^{*}=\\arg\\min_{G,E}\\max_{D}\\ \\mathcal{L}_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}(G,\\ D)+\\lambda_{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}\\mathcal{L}_{1}^{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}(G,\\ E)$\n",
        "\n",
        "<br>\n",
        "<h3>where</h3>\n",
        "$$\\mathcal{L}_{1}^{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}(G,\\ E)=\\mathrm{E}_{\\mathrm{A}\\sim p(\\mathrm{A}),\\mathrm{z}\\sim p(\\mathrm{z})}||\\mathrm{z}-E(G(\\mathrm{A},\\ \\mathrm{z}))||_{1}$$\n",
        "\n",
        "* $\\hat{\\mathrm{z}}=E(G(\\mathrm{A},\\ \\mathrm{z}))$ is encouraged to be close to the randomly drawn $\\mathrm{z}$ to enable bijective mapping.\n",
        "<br><br>\n",
        "<h6>\n",
        "* The discriminator loss $L_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}(G,\\ D)$  on $\\hat{\\mathrm{B}}$ is used to encourage the network to generate realistic results.\n",
        "</h6>\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "> <h2>Hybrid Model: BicycleGAN</h2>\n",
        "\n",
        "Combine the cVAE-GAN and cLR-GAN objectives in the hybrid model.<br><br>\n",
        "Training is done in both directions, aiming to take advantage of both cycles<br> ($\\mathrm{B}\\rightarrow \\mathrm{z}\\rightarrow\\hat{\\mathrm{B}}$ and $\\mathrm{z}\\rightarrow\\hat{\\mathrm{B}}\\rightarrow\\hat{\\mathrm{z}}$), hence the name BicycleGAN.<br><br>\n",
        "\n",
        "> <h4>Combined Objective</h4>\n",
        "\n",
        "<br>\n",
        "$$\n",
        "G^{*},\\ E^{*}=\\arg\\min_{G,E}\\max\\ \\mathcal{L}_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}^{\\mathrm{V}\\mathrm{A}\\mathrm{E}}(G,\\ D,\\ E)+\\lambda \\mathcal{L}_{1^{\\mathrm{A}\\mathrm{E}}}(G,\\ E)\n",
        "$$\n",
        "$$\n",
        "+\\mathcal{L}_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}(G,\\ D)+\\lambda_{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}\\mathcal{L}_{1}^{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}(G,\\ E)+\\lambda_{\\mathrm{K}\\mathrm{L}}\\mathcal{L}_{\\mathrm{K}\\mathrm{L}}(E)\\ ,\n",
        "$$\n",
        "<h5>\n",
        "where the hyper-parameters $\\lambda, \\lambda_{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}$, and $\\lambda_{\\mathrm{K}\\mathrm{L}}$ control the relative importance of each term.<h5>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "> <h2>Implementation details<h2> \n",
        "\n",
        "<h4>Network architecture<h4>\n",
        "\n",
        "* For generator G, U-Net is used, which contains an encoder-decoder\n",
        "architecture, with symmetric skip connections. The architecture has been shown to produce strong results in the unimodal image prediction setting when there is a spatial correspondence between input and output pairs.<br><br>\n",
        "![U-Net](img/unet.png)<br><br>\n",
        "* For discriminator D, generally two PatchGAN discriminators at different\n",
        "scales are used, which aim to predict real vs. fake overlapping image patches.\n",
        "<br><br>\n",
        "* For the encoder E, these two networks are preferred: <br>\n",
        "(1) $E_{CNN}$: CNN with a few convolutional and downsampling layers .<br>\n",
        "(2) $E_{Resnet}$: a classifier with several residual block .\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br><br>\n",
        "> <h2>Implementation in Pytorch<h2>\n",
        "\n",
        "<br>\n",
        "<h4>Dataset : Edges2Shoes</h4>\n",
        "\n",
        "<br>\n",
        "\n",
        "![Edges2Shoes](img/edgesToShoes.png)\n",
        "<br>\n",
        "\n",
        "<h5>Let us first import the required libraries</h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbzqqCBmnkVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "from  PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as Transforms\n",
        "from  torch.utils.data import Dataset\n",
        "from  torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjSSd6DiTkrI",
        "colab_type": "text"
      },
      "source": [
        "<h4>Dataloader</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_VmVI6RRrq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Edges2Shoes(Dataset):\n",
        "    def __init__(self, root, transform, mode='train'):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        \n",
        "        data_dir = os.path.join(root, mode)\n",
        "        self.file_list = os.listdir(data_dir)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root, self.mode, self.file_list[idx])\n",
        "        img = Image.open(img_path)\n",
        "        W, H = img.size[0], img.size[1]\n",
        "        \n",
        "        data = img.crop((0, 0, int(W / 2), H))\n",
        "        ground_truth = img.crop((int(W / 2), 0, W, H))\n",
        "        \n",
        "        data = self.transform(data)\n",
        "        ground_truth = self.transform(ground_truth)\n",
        "        \n",
        "        return (data, ground_truth)\n",
        "\n",
        "def data_loader(root, batch_size=1, shuffle=True, img_size=128, mode='train'):    \n",
        "    transform = Transforms.Compose([Transforms.Scale((img_size, img_size)),\n",
        "                                    Transforms.ToTensor(),\n",
        "                                    Transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
        "                                                         std=(0.5, 0.5, 0.5))\n",
        "                                   ])\n",
        "    \n",
        "    dset = Edges2Shoes(root, transform, mode=mode)\n",
        "    \n",
        "    if batch_size == 'all':\n",
        "        batch_size = len(dset)\n",
        "        \n",
        "    dloader = torch.utils.data.DataLoader(dset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=shuffle,\n",
        "                                          num_workers=0,\n",
        "                                          drop_last=True)\n",
        "    dlen = len(dset)\n",
        "    \n",
        "    return dloader, dlen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-JUwSFnU0lQ",
        "colab_type": "text"
      },
      "source": [
        "> <h3>ConvBlock</h3>\n",
        "\n",
        "<br>\n",
        "<h5>Small unit block consists of  (convolution layer - normalization layer - non linearity layer)</h5>\n",
        "<br>\n",
        "<i>Parameters</i><h5>\n",
        "\n",
        "    1. in_dim : Input dimension(channels number)\n",
        "    2. out_dim : Output dimension(channels number)\n",
        "    3. k : Kernel size(filter size)\n",
        "    4. s : stride\n",
        "    5. p : padding size\n",
        "    6. norm : If it is true add Instance Normalization layer, otherwise skip this layer\n",
        "    7. non_linear : You can choose between 'leaky_relu', 'relu', 'None'\n",
        "</hs>\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RjkwRL_T4YN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, k=4, s=2, p=1, norm=True, non_linear='leaky_relu'):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        # Convolution Layer\n",
        "        layers += [nn.Conv2d(in_dim, out_dim, kernel_size=k, stride=s, padding=p)]\n",
        "        \n",
        "        # Normalization Layer\n",
        "        if norm is True:\n",
        "            layers += [nn.InstanceNorm2d(out_dim, affine=True)]\n",
        "            \n",
        "        # Non-linearity Layer\n",
        "        if non_linear == 'leaky_relu':\n",
        "            layers += [nn.LeakyReLU(negative_slope=0.2, inplace=True)]\n",
        "        elif non_linear == 'relu':\n",
        "            layers += [nn.ReLU(inplace=True)]\n",
        "        \n",
        "        self.conv_block = nn.Sequential(* layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv_block(x)\n",
        "        return out\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XZ_KnX_XVrZ",
        "colab_type": "text"
      },
      "source": [
        "<h3>DeconvBlock</h3>\n",
        "<h5>Small unit block consists of (transpose conv layer - normalization layer - non linearity layer)\n",
        "    \n",
        "<i>Parameters</i>\n",
        "\n",
        "    1. in_dim : Input dimension(channels number)\n",
        "    2. out_dim : Output dimension(channels number)\n",
        "    3. k : Kernel size(filter size)\n",
        "    4. s : stride\n",
        "    5. p : padding size\n",
        "    6. norm : If it is true add Instance Normalization layer, otherwise skip this layer\n",
        "    7. non_linear : You can choose between 'relu', 'tanh', None\n",
        "</h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwJSqz1nW5QI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeconvBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, k=4, s=2, p=1, norm=True, non_linear='relu'):\n",
        "        super(DeconvBlock, self).__init__()\n",
        "        layers = []\n",
        "        \n",
        "        # Transpose Convolution Layer\n",
        "        layers += [nn.ConvTranspose2d(in_dim, out_dim, kernel_size=k, stride=s, padding=p)]\n",
        "        \n",
        "        # Normalization Layer\n",
        "        if norm is True:\n",
        "            layers += [nn.InstanceNorm2d(out_dim, affine=True)]\n",
        "        \n",
        "        # Non-Linearity Layer\n",
        "        if non_linear == 'relu':\n",
        "            layers += [nn.ReLU(inplace=True)]\n",
        "        elif non_linear == 'tanh':\n",
        "            layers += [nn.Tanh()]\n",
        "            \n",
        "        self.deconv_block = nn.Sequential(* layers)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        out = self.deconv_block(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mXlQaimYJIH",
        "colab_type": "text"
      },
      "source": [
        "<h2>Generator</h2>\n",
        "\n",
        "> U-Net Generator \n",
        "\n",
        "<br><h5>\n",
        "Downsampled activation volume and upsampled activation volume which have same width and height make pairs and they are concatenated when upsampling.<br></h5>\n",
        "\n",
        "    Pairs : (up_1, down_6)\n",
        "            (up_2, down_5)  \n",
        "            (up_3, down_4) \n",
        "            (up_4, down_3) \n",
        "            (up_5, down_2) \n",
        "            (up_6, down_1)\n",
        "            down_7 doesn't have a partener.\n",
        "<br><h5>\n",
        "ex) up_1 and down_6 have same size of  (N, 512, 2, 2) given that input size is (N, 3, 128, 128). When forwarding into upsample_2, up_1 and down_6 are concatenated to make (N, 1024, 2, 2) and then upsample_2 makes (N, 512, 4, 4). That is why upsample_2 has 1024 input dimension and 512 output dimension .\n",
        "Except upsample_1, all the other upsampling blocks do the same thing.</h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5lLjo3PYFWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=8): \n",
        "        super(Generator, self).__init__()\n",
        "        # Reduce H and W by half at every downsampling\n",
        "        self.downsample_1 = ConvBlock(3 + z_dim, 64, k=4, s=2, p=1, norm=False, non_linear='leaky_relu')\n",
        "        self.downsample_2 = ConvBlock(64, 128, k=4, s=2, p=1, norm=True, non_linear='leaky_relu')\n",
        "        self.downsample_3 = ConvBlock(128, 256, k=4, s=2, p=1, norm=True, non_linear='leaky_relu')\n",
        "        self.downsample_4 = ConvBlock(256, 512, k=4, s=2, p=1, norm=True, non_linear='leaky_relu')\n",
        "        self.downsample_5 = ConvBlock(512, 512, k=4, s=2, p=1, norm=True, non_linear='leaky_relu')\n",
        "        self.downsample_6 = ConvBlock(512, 512, k=4, s=2, p=1, norm=True, non_linear='leaky_relu')\n",
        "        self.downsample_7 = ConvBlock(512, 512, k=4, s=2, p=1, norm=True, non_linear='leaky_relu')\n",
        "        \n",
        "        # Need concatenation when upsampling, see foward function for details\n",
        "        self.upsample_1 = DeconvBlock(512, 512, k=4, s=2, p=1, norm=True, non_linear='relu')\n",
        "        self.upsample_2 = DeconvBlock(1024, 512, k=4, s=2, p=1, norm=True, non_linear='relu')\n",
        "        self.upsample_3 = DeconvBlock(1024, 512, k=4, s=2, p=1, norm=True, non_linear='relu')\n",
        "        self.upsample_4 = DeconvBlock(1024, 256, k=4, s=2, p=1, norm=True, non_linear='relu')\n",
        "        self.upsample_5 = DeconvBlock(512, 128, k=4, s=2, p=1, norm=True, non_linear='relu')\n",
        "        self.upsample_6 = DeconvBlock(256, 64, k=4, s=2, p=1, norm=True, non_linear='relu')\n",
        "        self.upsample_7 = DeconvBlock(128, 3, k=4, s=2, p=1, norm=False, non_linear='Tanh')\n",
        "    \n",
        "    def forward(self, x, z):\n",
        "        # z : (N, z_dim) -> (N, z_dim, 1, 1) -> (N, z_dim, H, W)\n",
        "        # x_with_z : (N, 3 + z_dim, H, W)\n",
        "        z = z.unsqueeze(dim=2).unsqueeze(dim=3)\n",
        "        z = z.expand(z.size(0), z.size(1), x.size(2), x.size(3))\n",
        "        x_with_z = torch.cat([x, z], dim=1)\n",
        "        \n",
        "        down_1 = self.downsample_1(x_with_z)\n",
        "        down_2 = self.downsample_2(down_1)\n",
        "        down_3 = self.downsample_3(down_2)\n",
        "        down_4 = self.downsample_4(down_3)\n",
        "        down_5 = self.downsample_5(down_4)\n",
        "        down_6 = self.downsample_6(down_5)\n",
        "        down_7 = self.downsample_7(down_6)\n",
        "\n",
        "        up_1 = self.upsample_1(down_7)\n",
        "        up_2 = self.upsample_2(torch.cat([up_1, down_6], dim=1))\n",
        "        up_3 = self.upsample_3(torch.cat([up_2, down_5], dim=1))\n",
        "        up_4 = self.upsample_4(torch.cat([up_3, down_4], dim=1))\n",
        "        up_5 = self.upsample_5(torch.cat([up_4, down_3], dim=1))\n",
        "        up_6 = self.upsample_6(torch.cat([up_5, down_2], dim=1))\n",
        "        out = self.upsample_7(torch.cat([up_6, down_1], dim=1))\n",
        "        \n",
        "        return out \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAdruM9yab27",
        "colab_type": "text"
      },
      "source": [
        "<h2> Discriminator </h2>\n",
        "\n",
        "> PatchGAN discriminator <b>:</b> \n",
        "\n",
        "<br><h5>\n",
        "  It uses two discriminator which have different output sizes (different local  probabilities).\n",
        "\n",
        "    d_1 : (N, 3, 128, 128) -> (N, 1, 14, 14)\n",
        "    d_2 : (N, 3, 128, 128) -> (N, 1, 30, 30)\n",
        "\n",
        "In training, the generator needs to fool both of d_1 and d_2 and it makes the generator more robust."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTY0_9yUaUc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()       \n",
        "        # Discriminator with last patch (14x14)\n",
        "        # (N, 3, 128, 128) -> (N, 1, 14, 14)\n",
        "        self.d_1 = nn.Sequential(nn.AvgPool2d(kernel_size=3, stride=2, padding=0, count_include_pad=False),\n",
        "                                 ConvBlock(3, 32, k=4, s=2, p=1, norm=False, non_linear='leaky_relu'),\n",
        "                                 ConvBlock(32, 64, k=4, s=2, p=1, norm=True, non_linear='leaky-relu'),\n",
        "                                 ConvBlock(64, 128, k=4, s=1, p=1, norm=True, non_linear='leaky-relu'),\n",
        "                                 ConvBlock(128, 1, k=4, s=1, p=1, norm=False, non_linear=None))\n",
        "        \n",
        "        # Discriminator with last patch (30x30)\n",
        "        # (N, 3, 128, 128) -> (N, 1, 30, 30)\n",
        "        self.d_2 = nn.Sequential(ConvBlock(3, 64, k=4, s=2, p=1, norm=False, non_linear='leaky_relu'),\n",
        "                                 ConvBlock(64, 128, k=4, s=2, p=1, norm=True, non_linear='leaky-relu'),\n",
        "                                 ConvBlock(128, 256, k=4, s=1, p=1, norm=True, non_linear='leaky-relu'),\n",
        "                                 ConvBlock(256, 1, k=4, s=1, p=1, norm=False, non_linear=None))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out_1 = self.d_1(x)\n",
        "        out_2 = self.d_2(x)\n",
        "        return (out_1, out_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShBv1Xh7bext",
        "colab_type": "text"
      },
      "source": [
        "<h2>ResBlock</h2>\n",
        "    \n",
        "This residual block is different with the one we usaully know which consists of<br>[conv - norm - act - conv - norm] and identity mapping(x -> x) for shortcut. Also spatial size is decreased by half because of AvgPool2d."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds4-gC1Iba1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv = nn.Sequential(nn.InstanceNorm2d(in_dim, affine=True),\n",
        "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                                  nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=1, padding=1),\n",
        "                                  nn.InstanceNorm2d(in_dim, affine=True),\n",
        "                                  nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                                  nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "                                  nn.AvgPool2d(kernel_size=2, stride=2, padding=0))\n",
        "        \n",
        "        self.short_cut = nn.Sequential(nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n",
        "                                       nn.Conv2d(in_dim, out_dim, kernel_size=1, stride=1, padding=0))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.conv(x) + self.short_cut(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKtOPieCU9DE",
        "colab_type": "text"
      },
      "source": [
        "<h2>Encoder</h2><h5>\n",
        "Output is mu and log(var) for reparameterization trick used in Variation Auto Encoder.<br>Encoding is done in this order.\n",
        "\n",
        "    1. Use this encoder and get mu and log_var\n",
        "    2. std = exp(log(var / 2))\n",
        "    3. random_z = N(0, 1)\n",
        "    4. encoded_z = random_z * std + mu (Reparameterization trick)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPjnXSk0D-BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, z_dim=8):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.conv = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.res_blocks = nn.Sequential(ResBlock(64, 128),\n",
        "                                        ResBlock(128, 192),\n",
        "                                        ResBlock(192, 256))\n",
        "        self.pool_block = nn.Sequential(nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "                                        nn.AvgPool2d(kernel_size=8, stride=8, padding=0))\n",
        "        \n",
        "        # Return mu and logvar for reparameterization trick\n",
        "        self.fc_mu = nn.Linear(256, z_dim)\n",
        "        self.fc_logvar = nn.Linear(256, z_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # (N, 3, 128, 128) -> (N, 64, 64, 64)\n",
        "        out = self.conv(x)\n",
        "        # (N, 64, 64, 64) -> (N, 128, 32, 32) -> (N, 192, 16, 16) -> (N, 256, 8, 8)\n",
        "        out = self.res_blocks(out)\n",
        "        # (N, 256, 8, 8) -> (N, 256, 1, 1)\n",
        "        out = self.pool_block(out)\n",
        "        # (N, 256, 1, 1) -> (N, 256)\n",
        "        out = out.view(x.size(0), -1)\n",
        "        \n",
        "        # (N, 256) -> (N, z_dim) x 2\n",
        "        mu = self.fc_mu(out)\n",
        "        log_var = self.fc_logvar(out)\n",
        "        \n",
        "        return (mu, log_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeGdLiOiV7Hu",
        "colab_type": "text"
      },
      "source": [
        "<h3>Function var</h3>\n",
        "<h4>Convert tensor to Variable</h4>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWA-k0c0VhKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def var(tensor, requires_grad=True):\n",
        "    if torch.cuda.is_available():\n",
        "        dtype = torch.cuda.FloatTensor\n",
        "    else:\n",
        "        dtype = torch.FloatTensor\n",
        "        \n",
        "    var = Variable(tensor.type(dtype), requires_grad=requires_grad)\n",
        "    \n",
        "    return var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KteKdFg_Wguf",
        "colab_type": "text"
      },
      "source": [
        "<h3>Function make_img </h3>\n",
        "\n",
        "* <h4>Generate images</h4>\n",
        "\n",
        "<h5>Parameters\n",
        "\n",
        "    dloader : Data loader for test data set\n",
        "    G : Generator\n",
        "    z : random_z(size = (N, img_num, z_dim))\n",
        "    N : test img number / img_num : Number of images that you want to generate with one test img / z_dim : 8\n",
        "    img_num : Number of images that you want to generate with one test img\n",
        "</h5>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_8tSfXkWewb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_img(dloader, G, z, img_num=5, img_size=128):\n",
        "    if torch.cuda.is_available():\n",
        "        dtype = torch.cuda.FloatTensor\n",
        "    else:\n",
        "        dtype = torch.FloatTensor\n",
        "        \n",
        "    dloader = iter(dloader)\n",
        "    img, _ = dloader.next()\n",
        "\n",
        "    N = img.size(0)    \n",
        "    img = var(img.type(dtype))\n",
        "\n",
        "    result_img = torch.FloatTensor(N * (img_num + 1), 3, img_size, img_size).type(dtype)\n",
        "\n",
        "    for i in range(N):\n",
        "        # original image to the leftmost\n",
        "        result_img[i * (img_num + 1)] = img[i].data\n",
        "\n",
        "        # Insert generated images to the next of the original image\n",
        "        for j in range(img_num):\n",
        "            img_ = img[i].unsqueeze(dim=0)\n",
        "            z_ = z[i, j, :].unsqueeze(dim=0)\n",
        "            \n",
        "            out_img = G(img_, z_)\n",
        "            result_img[i * (img_num + 1) + j + 1] = out_img.data\n",
        "\n",
        "\n",
        "    # [-1, 1] -> [0, 1]\n",
        "    result_img = result_img / 2 + 0.5\n",
        "    \n",
        "    return result_img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96MfCKJgYO3g",
        "colab_type": "text"
      },
      "source": [
        "<h2>mse_loss</h2>\n",
        "\n",
        "* Calculate mean squared error loss\n",
        "\n",
        "<h5>Parameters\n",
        "    \n",
        "    score : Output of discriminator\n",
        "    target : 1 for real and 0 for fake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGu9PpaqYNKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mse_loss(score, target=1):\n",
        "    dtype = type(score)\n",
        "    \n",
        "    if target == 1:\n",
        "        label = util.var(torch.ones(score.size()), requires_grad=False)\n",
        "    elif target == 0:\n",
        "        label = util.var(torch.zeros(score.size()), requires_grad=False)\n",
        "    \n",
        "    criterion = nn.MSELoss()\n",
        "    loss = criterion(score, label)\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXOLDkENZLZT",
        "colab_type": "text"
      },
      "source": [
        "<h2>L1_loss</h2>\n",
        "\n",
        "* Calculate L1 loss\n",
        "\n",
        "<h5>Parameters\n",
        "\n",
        "    pred : Output of network\n",
        "    target : Ground truth\n",
        "</h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGiCJ8qTZJr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L1_loss(pred, target):\n",
        "    return torch.mean(torch.abs(pred - target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m9GL_tKZhwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_decay_rule(epoch, start_decay=100, lr_decay=100):\n",
        "    decay_rate = 1.0 - (max(0, epoch - start_decay) / float(lr_decay))\n",
        "    return decay_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxv2mPI5chtZ",
        "colab_type": "text"
      },
      "source": [
        "<h3> Solver function</h3>\n",
        "\n",
        "* <h5>Include all the training details \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol3Pci0KZp5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Solver():\n",
        "    def __init__(self, root='data/edges2shoes', result_dir='result', weight_dir='weight', load_weight=False,\n",
        "                 batch_size=2, test_size=20, test_img_num=5, img_size=128, num_epoch=100, save_every=1000,\n",
        "                 lr=0.0002, beta_1=0.5, beta_2=0.999, lambda_kl=0.01, lambda_img=10, lambda_z=0.5, z_dim=8):\n",
        "        \n",
        "        # Data type(Can use GPU or not?)\n",
        "        self.dtype = torch.cuda.FloatTensor\n",
        "        if torch.cuda.is_available() is False:\n",
        "            self.dtype = torch.FloatTensor\n",
        "        \n",
        "        # Data loader for training\n",
        "        self.dloader, dlen = data_loader(root=root, batch_size=batch_size, shuffle=True, \n",
        "                                         img_size=img_size, mode='train')\n",
        "\n",
        "        # Data loader for test\n",
        "        self.t_dloader, _ = data_loader(root=root, batch_size=test_size, shuffle=False, \n",
        "                                        img_size=img_size, mode='val')\n",
        "\n",
        "        # Both of D_cVAE and D_cLR has two discriminators which have different output size((14x14) and (30x30)).\n",
        "        # Totally, we have for discriminators now.\n",
        "        self.D_cVAE = model.Discriminator().type(self.dtype)\n",
        "        self.D_cLR = model.Discriminator().type(self.dtype)\n",
        "        self.G = model.Generator(z_dim=z_dim).type(self.dtype)\n",
        "        self.E = model.Encoder(z_dim=z_dim).type(self.dtype)\n",
        "\n",
        "        # Optimizers\n",
        "        self.optim_D_cVAE = optim.Adam(self.D_cVAE.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "        self.optim_D_cLR = optim.Adam(self.D_cLR.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "        self.optim_G = optim.Adam(self.G.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "        self.optim_E = optim.Adam(self.E.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "\n",
        "        # fixed random_z for intermediate test\n",
        "        self.fixed_z = util.var(torch.randn(test_size, test_img_num, z_dim))\n",
        "        \n",
        "        # Some hyperparameters\n",
        "        self.z_dim = z_dim\n",
        "        self.lambda_kl = lambda_kl\n",
        "        self.lambda_img = lambda_img\n",
        "        self.lambda_z = lambda_z\n",
        "\n",
        "        # Extra things\n",
        "        self.result_dir = result_dir\n",
        "        self.weight_dir = weight_dir\n",
        "        self.load_weight = load_weight\n",
        "        self.test_img_num = test_img_num\n",
        "        self.img_size = img_size\n",
        "        self.start_epoch = 0\n",
        "        self.num_epoch = num_epoch\n",
        "        self.save_every = save_every\n",
        "        \n",
        "    '''\n",
        "        < set_train_phase >\n",
        "        Set training phase\n",
        "    '''\n",
        "    def set_train_phase(self):\n",
        "        self.D_cVAE.train()\n",
        "        self.D_cLR.train()\n",
        "        self.G.train()\n",
        "        self.E.train()\n",
        "        \n",
        "    '''\n",
        "        < load_pretrained >\n",
        "        If you want to continue to train, load pretrained weight\n",
        "    '''\n",
        "    def load_pretrained(self):\n",
        "        self.D_cVAE.load_state_dict(torch.load(os.path.join(self.weight_dir, 'D_cVAE.pkl')))\n",
        "        self.D_cLR.load_state_dict(torch.load(os.path.join(self.weight_dir, 'D_cLR.pkl')))\n",
        "        self.G.load_state_dict(torch.load(os.path.join(self.weight_dir, 'G.pkl')))\n",
        "        self.E.load_state_dict(torch.load(os.path.join(self.weight_dir, 'E.pkl')))\n",
        "        \n",
        "        log_file = open('log.txt', 'r')\n",
        "        line = log_file.readline()\n",
        "        self.start_epoch = int(line)\n",
        "        \n",
        "    '''\n",
        "        < save_weight >\n",
        "        Save weight\n",
        "    '''\n",
        "    def save_weight(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            d_cVAE_name = 'D_cVAE.pkl'\n",
        "            d_cLR_name = 'D_cLR.pkl'\n",
        "            g_name = 'G.pkl'\n",
        "            e_name = 'E.pkl'\n",
        "        else:\n",
        "            d_cVAE_name = '{epochs}-{name}'.format(epochs=str(epoch), name='D_cVAE.pkl')\n",
        "            d_cLR_name = '{epochs}-{name}'.format(epochs=str(epoch), name='D_cLR.pkl')\n",
        "            g_name = '{epochs}-{name}'.format(epochs=str(epoch), name='G.pkl')\n",
        "            e_name = '{epochs}-{name}'.format(epochs=str(epoch), name='E.pkl')\n",
        "            \n",
        "        torch.save(self.D_cVAE.state_dict(), os.path.join(self.weight_dir, d_cVAE_name))\n",
        "        torch.save(self.D_cVAE.state_dict(), os.path.join(self.weight_dir, d_cLR_name))\n",
        "        torch.save(self.G.state_dict(), os.path.join(self.weight_dir, g_name))\n",
        "        torch.save(self.E.state_dict(), os.path.join(self.weight_dir, e_name))\n",
        "    \n",
        "    '''\n",
        "        < all_zero_grad >\n",
        "        Set all optimizers' grad to zero \n",
        "    '''\n",
        "    def all_zero_grad(self):\n",
        "        self.optim_D_cVAE.zero_grad()\n",
        "        self.optim_D_cLR.zero_grad()\n",
        "        self.optim_G.zero_grad()\n",
        "        self.optim_E.zero_grad()\n",
        "        \n",
        "    '''\n",
        "        < train >\n",
        "        Train the D_cVAE, D_cLR, G and E \n",
        "    '''\n",
        "    def train(self):\n",
        "        if self.load_weight is True:\n",
        "            self.load_pretrained()\n",
        "        \n",
        "        self.set_train_phase()\n",
        "        \n",
        "        for epoch in range(self.start_epoch, self.num_epoch):\n",
        "            for iters, (img, ground_truth) in enumerate(self.dloader):\n",
        "                # img : (2, 3, 128, 128) of domain A / ground_truth : (2, 3, 128, 128) of domain B\n",
        "                img, ground_truth = util.var(img), util.var(ground_truth)\n",
        "\n",
        "                # Seperate data for cVAE_GAN and cLR_GAN\n",
        "                cVAE_data = {'img' : img[0].unsqueeze(dim=0), 'ground_truth' : ground_truth[0].unsqueeze(dim=0)}\n",
        "                cLR_data = {'img' : img[1].unsqueeze(dim=0), 'ground_truth' : ground_truth[1].unsqueeze(dim=0)}\n",
        "\n",
        "                ''' ----------------------------- 1. Train D ----------------------------- '''\n",
        "                #############   Step 1. D loss in cVAE-GAN #############\n",
        "\n",
        "                # Encoded latent vector\n",
        "                mu, log_variance = self.E(cVAE_data['ground_truth'])\n",
        "                std = torch.exp(log_variance / 2)\n",
        "                random_z = util.var(torch.randn(1, self.z_dim))\n",
        "                encoded_z = (random_z * std) + mu\n",
        "\n",
        "                # Generate fake image\n",
        "                fake_img_cVAE = self.G(cVAE_data['img'], encoded_z)\n",
        "\n",
        "                # Get scores and loss\n",
        "                real_d_cVAE_1, real_d_cVAE_2 = self.D_cVAE(cVAE_data['ground_truth'])\n",
        "                fake_d_cVAE_1, fake_d_cVAE_2 = self.D_cVAE(fake_img_cVAE)\n",
        "                \n",
        "                # mse_loss for LSGAN\n",
        "                D_loss_cVAE_1 = mse_loss(real_d_cVAE_1, 1) + mse_loss(fake_d_cVAE_1, 0)\n",
        "                D_loss_cVAE_2 = mse_loss(real_d_cVAE_2, 1) + mse_loss(fake_d_cVAE_2, 0)\n",
        "                \n",
        "                #############   Step 2. D loss in cLR-GAN   #############\n",
        "\n",
        "                # Random latent vector\n",
        "                random_z = util.var(torch.randn(1, self.z_dim))\n",
        "\n",
        "                # Generate fake image\n",
        "                fake_img_cLR = self.G(cLR_data['img'], random_z)\n",
        "\n",
        "                # Get scores and loss\n",
        "                real_d_cLR_1, real_d_cLR_2 = self.D_cLR(cLR_data['ground_truth'])\n",
        "                fake_d_cLR_1, fake_d_cLR_2 = self.D_cLR(fake_img_cLR)\n",
        "                \n",
        "                D_loss_cLR_1 = mse_loss(real_d_cLR_1, 1) + mse_loss(fake_d_cLR_1, 0)\n",
        "                D_loss_cLR_2 = mse_loss(real_d_cLR_2, 1) + mse_loss(fake_d_cLR_2, 0)\n",
        "\n",
        "                D_loss = D_loss_cVAE_1 + D_loss_cLR_1 + D_loss_cVAE_2 + D_loss_cLR_2\n",
        "\n",
        "                # Update\n",
        "                self.all_zero_grad()\n",
        "                D_loss.backward()\n",
        "                self.optim_D_cVAE.step()\n",
        "                self.optim_D_cLR.step()\n",
        "\n",
        "                ''' ----------------------------- 2. Train G & E ----------------------------- '''\n",
        "                ############# Step 1. GAN loss to fool discriminator (cVAE_GAN and cLR_GAN) #############\n",
        "\n",
        "                # Encoded latent vector\n",
        "                mu, log_variance = self.E(cVAE_data['ground_truth'])\n",
        "                std = torch.exp(log_variance / 2)\n",
        "                random_z = util.var(torch.randn(1, self.z_dim))\n",
        "                encoded_z = (random_z * std) + mu\n",
        "\n",
        "                # Generate fake image and get adversarial loss\n",
        "                fake_img_cVAE = self.G(cVAE_data['img'], encoded_z)\n",
        "                fake_d_cVAE_1, fake_d_cVAE_2 = self.D_cVAE(fake_img_cVAE)\n",
        "\n",
        "                GAN_loss_cVAE_1 = mse_loss(fake_d_cVAE_1, 1)\n",
        "                GAN_loss_cVAE_2 = mse_loss(fake_d_cVAE_2, 1)\n",
        "\n",
        "                # Random latent vector\n",
        "                random_z = util.var(torch.randn(1, self.z_dim))\n",
        "\n",
        "                # Generate fake image and get adversarial loss\n",
        "                fake_img_cLR = self.G(cLR_data['img'], random_z)\n",
        "                fake_d_cLR_1, fake_d_cLR_2 = self.D_cLR(fake_img_cLR)\n",
        "\n",
        "                GAN_loss_cLR_1 = mse_loss(fake_d_cLR_1, 1)\n",
        "                GAN_loss_cLR_2 = mse_loss(fake_d_cLR_2, 1)\n",
        "\n",
        "                G_GAN_loss = GAN_loss_cVAE_1 + GAN_loss_cVAE_2 + GAN_loss_cLR_1 + GAN_loss_cLR_2\n",
        "\n",
        "                ############# Step 2. KL-divergence with N(0, 1) (cVAE-GAN) #############\n",
        "                \n",
        "                KL_div = self.lambda_kl * torch.sum(0.5 * (mu ** 2 + torch.exp(log_variance) - log_variance - 1))\n",
        "\n",
        "                ############# Step 3. Reconstruction of ground truth image (|G(A, z) - B|) (cVAE-GAN) #############\n",
        "                img_recon_loss = self.lambda_img * L1_loss(fake_img_cVAE, cVAE_data['ground_truth'])\n",
        "\n",
        "                EG_loss = G_GAN_loss + KL_div + img_recon_loss\n",
        "                self.all_zero_grad()\n",
        "                EG_loss.backward(retain_graph=True)\n",
        "                self.optim_E.step()\n",
        "                self.optim_G.step()\n",
        "\n",
        "                ''' ----------------------------- 3. Train ONLY G ----------------------------- '''\n",
        "                ############ Step 1. Reconstrution of random latent code (|E(G(A, z)) - z|) (cLR-GAN) ############\n",
        "                \n",
        "                # This step should update ONLY G.\n",
        "                mu_, log_variance_ = self.E(fake_img_cLR)\n",
        "                z_recon_loss = L1_loss(mu_, random_z)\n",
        "\n",
        "                G_alone_loss = self.lambda_z * z_recon_loss\n",
        "\n",
        "                self.all_zero_grad()\n",
        "                G_alone_loss.backward()\n",
        "                self.optim_G.step()\n",
        "\n",
        "                log_file = open('log.txt', 'w')\n",
        "                log_file.write(str(epoch))\n",
        "                \n",
        "                # Print error and save intermediate result image and weight\n",
        "                if iters % self.save_every == 0:\n",
        "                    print('[Epoch : %d / Iters : %d] => D_loss : %f / G_GAN_loss : %f / KL_div : %f / img_recon_loss : %f / z_recon_loss : %f'\\\n",
        "                          %(epoch, iters, D_loss.data[0], G_GAN_loss.data[0], KL_div.data[0], img_recon_loss.data[0], G_alone_loss.data[0]))\n",
        "\n",
        "                    # Save intermediate result image\n",
        "                    if os.path.exists(self.result_dir) is False:\n",
        "                        os.makedirs(self.result_dir)\n",
        "\n",
        "                    result_img = util.make_img(self.t_dloader, self.G, self.fixed_z, \n",
        "                                               img_num=self.test_img_num, img_size=self.img_size)\n",
        "\n",
        "                    img_name = '{epoch}_{iters}.png'.format(epoch=epoch, iters=iters)\n",
        "                    img_path = os.path.join(self.result_dir, img_name)\n",
        "\n",
        "                    torchvision.utils.save_image(result_img, img_path, nrow=self.test_img_num+1)\n",
        "\n",
        "                    # Save intermediate weight\n",
        "                    if os.path.exists(self.weight_dir) is False:\n",
        "                        os.makedirs(self.weight_dir)\n",
        "                    \n",
        "                    self.save_weight()\n",
        "                    \n",
        "            # Save weight at the end of every epoch\n",
        "            self.save_weight(epoch=epoch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz1CeIZreEli",
        "colab_type": "text"
      },
      "source": [
        "<h3>Resources</h3>\n",
        "\n",
        "* [Toward Multimodal Image-to-Image Translation](https://arxiv.org/abs/1711.11586)\n",
        "* https://github.com/eveningglow/BicycleGAN-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxQPC3wkdjw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}